{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use the GPU: GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():  \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    \n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv(\"./train.csv\")\n",
    "df_test=pd.read_csv(\"./test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess(text):\n",
    "\n",
    "    text=text.lower()\n",
    "    # remove hyperlinks\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    text = re.sub(r'http?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    #Replace &amp, &lt, &gt with &,<,> respectively\n",
    "    text=text.replace(r'&amp;?',r'and')\n",
    "    text=text.replace(r'&lt;',r'<')\n",
    "    text=text.replace(r'&gt;',r'>')\n",
    "    #remove hashtag sign\n",
    "    #text=re.sub(r\"#\",\"\",text)   \n",
    "    #remove mentions\n",
    "    text = re.sub(r\"(?:\\@)\\w+\", '', text)\n",
    "    #text=re.sub(r\"@\",\"\",text)\n",
    "    #remove non ascii chars\n",
    "    text=text.encode(\"ascii\",errors=\"ignore\").decode()\n",
    "    #remove some puncts (except . ! ?)\n",
    "    text=re.sub(r'[:\"#$%&\\*+,-/:;<=>@\\\\^_`{|}~]+','',text)\n",
    "    text=re.sub(r'[!]+','!',text)\n",
    "    text=re.sub(r'[?]+','?',text)\n",
    "    text=re.sub(r'[.]+','.',text)\n",
    "    text=re.sub(r\"'\",\"\",text)\n",
    "    text=re.sub(r\"\\(\",\"\",text)\n",
    "    text=re.sub(r\"\\)\",\"\",text)\n",
    "    \n",
    "    text=\" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(preprocess)\n",
    "df_test['text'] = df_test['text'].apply(preprocess)\n",
    "df_train=df_train[df_train[\"text\"]!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  our deeds are the reason of this earthquake ma...       1\n",
       "1              forest fire near la ronge sask canada       1\n",
       "2  all residents asked to shelter in place are be...       1\n",
       "3  13000 people receive wildfires evacuation orde...       1\n",
       "4  just got sent this photo from ruby alaska as s...       1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=df_train[[\"text\",\"target\"]]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4314\n",
       "1    3247\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lists of lyrics and their labels.\n",
    "texts = df_train.text.values\n",
    "labels = df_train.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElectraForSequenceClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ElectraClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification,AdamW\n",
    "import torch\n",
    "tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\n",
    "model = ElectraForSequenceClassification.from_pretrained('google/electra-base-discriminator',num_labels=2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models\n",
    "import hiddenlayer as hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectraForSequenceClassification(\n",
      "  (electra): ElectraModel(\n",
      "    (embeddings): ElectraEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): ElectraEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): ElectraClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAE9CAYAAAD9MZD2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAdS0lEQVR4nO3de7BlZXnn8e9PLl4QbcBOF9NN2ygExkkCYkswOBZCzHBxaMZBwFJsGazOBQ1eZiJmMmM0SVVbZjRaSTA94tgYFQmKdAJJJIDRXEAaVFDQ0BKwu4dLSwBBBhR85o/1nrhpT5+zz+ne57LO91O1a6/1rnet9ewFu5/zrvXu901VIUmS5renzHYAkiRp55nQJUnqARO6JEk9YEKXJKkHTOiSJPWACV2SpB7YfbYD2BnPec5zasWKFbMdhiRJM+KGG274blUtHm/bSBN6krcCbwQKuBk4C9gfuAjYD7gBOLOqfpDkqcCFwIuA+4DTq+qOiY6/YsUKNm7cOLoPIEnSHJLkzh1tG9kt9yRLgV8HVlbVzwC7AWcA7wU+UFUHAfcDZ7ddzgbub+UfaPUkSdIQRv0MfXfg6Ul2B54B3AUcC1zStq8HTmnLq9o6bftxSTLi+CRJ6oWRJfSq2gr8PvAdukT+IN0t9geq6vFWbQuwtC0vBTa3fR9v9fcbVXySJPXJKG+570PX6j4Q+DfAXsDxu+C4a5JsTLJx27ZtO3s4SZJ6YZS33H8R+Oeq2lZVPwQ+CxwNLGq34AGWAVvb8lbgAIC2/dl0neOepKrWVdXKqlq5ePG4Hf0kSVpwRpnQvwMcleQZ7Vn4ccAtwDXAqa3OauCytryhrdO2X11OBSdJ0lBG+Qz9OrrObTfS/WTtKcA64B3A25JsontGfkHb5QJgv1b+NuC8UcUmSVLfZD43gleuXFn+Dl2StFAkuaGqVo63zaFfJUnqARO6JEk9YEKXJKkH5vXkLFoYVpx3+VD17lh70ogjkaS5yxa6JEk9YEKXJKkHTOiSJPWACV2SpB4woUuS1AMmdEmSesCELklSD5jQJUnqARO6JEk9YEKXJKkHHPpV2gGHnJU0n9hClySpB0zokiT1gAldkqQeMKFLktQDJnRJknrAhC5JUg+Y0CVJ6gETuiRJPWBClySpB0zokiT1wMgSepJDknx14PW9JG9Jsm+SK5Pc1t73afWT5ENJNiW5KckRo4pNkqS+GVlCr6pvVdXhVXU48CLgEeBS4Dzgqqo6GLiqrQOcABzcXmuA80cVmyRJfTNTt9yPA75dVXcCq4D1rXw9cEpbXgVcWJ1rgUVJ9p+h+CRJmtdmKqGfAXyqLS+pqrva8t3Akra8FNg8sM+WViZJkiYx8oSeZE/gZODPtt9WVQXUFI+3JsnGJBu3bdu2i6KUJGl+m4kW+gnAjVV1T1u/Z+xWenu/t5VvBQ4Y2G9ZK3uSqlpXVSurauXixYtHGLYkSfPHTCT01/Dj2+0AG4DVbXk1cNlA+etbb/ejgAcHbs1LkqQJ7D7KgyfZC3gF8MsDxWuBi5OcDdwJnNbKrwBOBDbR9Yg/a5SxSZLUJyNN6FX1fWC/7cruo+v1vn3dAs4ZZTySJPWVI8VJktQDJnRJknrAhC5JUg+Y0CVJ6gETuiRJPWBClySpB0zokiT1gAldkqQeMKFLktQDJnRJknrAhC5JUg+Y0CVJ6gETuiRJPWBClySpB0zokiT1gAldkqQeMKFLktQDJnRJknrAhC5JUg/sPtsBaHatOO/yoerdsfakEUciSdoZttAlSeoBE7okST1gQpckqQdM6JIk9cBIE3qSRUkuSfLNJLcmeUmSfZNcmeS29r5Pq5skH0qyKclNSY4YZWySJPXJqFvoHwT+qqoOBQ4DbgXOA66qqoOBq9o6wAnAwe21Bjh/xLFJktQbI0voSZ4NvAy4AKCqflBVDwCrgPWt2nrglLa8CriwOtcCi5LsP6r4JEnqk1G20A8EtgH/J8lXknwkyV7Akqq6q9W5G1jSlpcCmwf239LKJEnSJEY5sMzuwBHAm6vquiQf5Me31wGoqkpSUzlokjV0t+RZvnz5ropVmlMc8EfSVI2yhb4F2FJV17X1S+gS/D1jt9Lb+71t+1bggIH9l7WyJ6mqdVW1sqpWLl68eGTBS5I0n4wsoVfV3cDmJIe0ouOAW4ANwOpWthq4rC1vAF7fersfBTw4cGtekiRNYNRjub8Z+ESSPYHbgbPo/oi4OMnZwJ3Aaa3uFcCJwCbgkVZXkiQNYaQJvaq+CqwcZ9Nx49Qt4JxRxiNJUl85UpwkST1gQpckqQdM6JIk9YAJXZKkHjChS5LUAyZ0SZJ6wIQuSVIPmNAlSeoBE7okST1gQpckqQdM6JIk9YAJXZKkHjChS5LUAyZ0SZJ6wIQuSVIPmNAlSeoBE7okST1gQpckqQdM6JIk9YAJXZKkHjChS5LUAyZ0SZJ6YNKEnuToJHu15dcleX+S544+NEmSNKxhWujnA48kOQx4O/Bt4MKRRiVJkqZkmIT+eFUVsAr4w6r6I2DvYQ6e5I4kNyf5apKNrWzfJFcmua2979PKk+RDSTYluSnJEdP9UJIkLTTDJPSHkrwTeB1weZKnAHtM4Rwvr6rDq2plWz8PuKqqDgauausAJwAHt9caujsDkiRpCMMk9NOBx4Czq+puYBnwvp045ypgfVteD5wyUH5hda4FFiXZfyfOI0nSgjFMQn9rVb2/qr4EUFXfAf7dkMcv4PNJbkiyppUtqaq72vLdwJK2vBTYPLDvllYmSZImMUxCf8U4ZScMefyXVtURrf45SV42uLE9m68hjwVAkjVJNibZuG3btqnsKklSb+0woSf51SQ3A4e0Tmpjr38Gbh7m4FW1tb3fC1wKHAncM3Yrvb3f26pvBQ4Y2H1ZK9v+mOuqamVVrVy8ePEwYUiS1HsTtdA/CfxHYEN7H3u9qKpeO9mBk+yVZO+xZeCXgK+3461u1VYDl7XlDcDrW2/3o4AHB27NS5KkCey+ow1V9SDwIPCaJLvRPeveHXhmkme2Z+kTWQJcmmTsPJ+sqr9Kcj1wcZKzgTuB01r9K4ATgU3AI8BZ0/9YkiQtLDtM6GOSvAn4beAe4EetuICfm2i/qrodOGyc8vuA48YpL+CcSSOWJEk/YdKEDrwFOKQlYkmSNAcN08t9M92td0mSNEcN00K/HfhCksvpBpgBoKreP7KoJEnSlAyT0L/TXnu2lyRJmmMmTehV9W6AJM+oqkdGH5IkSZqqYeZDf0mSW4BvtvXDkvzxyCOTJElDG6ZT3B8A/wG4D6Cqvga8bMI9JEnSjBomoVNVm7cremIEsUiSpGkaplPc5iS/AFSSPYBzgVtHG5YkSZqKYVrov0I3gttSuslSDscR3SRJmlOG6eX+XWDSyVgkSdLsGWYs9wOBNwMrButX1cmjC0uSJE3FMM/QPwdcAPw5P56cRZIkzSHDJPRHq+pDI49EkiRN2zAJ/YNJ3gV8nieP5X7jyKKSJElTMkxC/1ngTOBYnjwf+rGjCkqSJE3NMAn91cDzquoHow5GkiRNzzC/Q/86sGjUgUiSpOkbpoW+CPhmkut58jN0f7YmSdIcMUxCf9fIo5AkSTtlmJHi/hYgybOGqS9JkmbeMCPFrQHeAzxK18s9dL3cnzfa0CRJ0rCGaXH/N+Bn2pjukiRpDhqml/u3gUdGHYgkSZq+YVro7wT+Icl1PLmX+6+PLCpJkjQlwyT0PwGuBm5mGpOzJNkN2AhsrapXttnbLgL2A24AzqyqHyR5KnAh8CLgPuD0qrpjqueTJGkhGiah71FVb9uJc5wL3Ao8q62/F/hAVV2U5MPA2cD57f3+qjooyRmt3uk7cV5JkhaMYZ6h/2WSNUn2T7Lv2GuYgydZBpwEfKSth24M+EtalfXAKW15VVunbT+u1ZckSZMYpoX+mvb+zoGyYX+29gfAbwB7t/X9gAeq6vG2vgVY2paXApsBqurxJA+2+vaulyRpEsMMLHPgdA6c5JXAvVV1Q5JjpnOMHRx3DbAGYPny5bvqsJIkzWvDDCyzB/CrwMta0ReAP6mqH06y69HAyUlOBJ5G9wz9g8CiJLu3VvoyYGurvxU4ANiSZHfg2XSd456kqtYB6wBWrlxZk8UvSdJCMMwz9PPpep7/cXu9qJVNqKreWVXLqmoFcAZwdVW9FrgGOLVVWw1c1pY3tHXa9quryoQtSdIQhnmG/uKqOmxg/eokX9uJc74DuCjJ7wJfAS5o5RcAH0+yCfgXuj8CJEnSEIZJ6E8keX5VfRsgyfOAJ6Zykqr6At2teqrqduDIceo8Crx6KseVJEmdYcdyvybJ7XQTszwXOGukUUmSpCkZppf7VUkOBg5pRd+qqscm2keSJM2sHSb0JK8DUlUfbwn8plZ+ZpInquqTMxWkJEma2ES93N8MXDpO+WeBt48mHEmSNB0TJfQ9qurh7Qur6vvAHqMLSZIkTdVECf3pSfbavjDJ3sCeowtJkiRN1UQJ/QLgkiTPHStIsoJu6tMLdrCPJEmaBTvsFFdVv5/kYeCLSZ7Zih8G1lbVpCPFSZKkmTPhz9aq6sPAh9ttdqrqoRmJSpqGFeddPlS9O9aeNOJIJGnmDTOwjIlckqQ5bqiELo3CsC1qSdLkdtgpLsmr2/u05kOXJEkzZ6Je7u9s75+ZiUAkSdL0TXTL/b4knwcOTLJh+41VdfLowpIkSVMxUUI/CTgC+Djwv2YmHEmSNB0T/Q79B8C1SX6hqraN/RZ9vOFgpfnEzniS+miYXu5L2q33fYEk2Qasrqqvjza0hWWu/4baJChJc9tEneLGrAPeVlXPrarldDOtrRttWJIkaSqGSeh7VdU1YytV9QXgJyZtkSRJs2eYW+63J/kfdJ3jAF4H3D66kKT5ZSqPIxx2VtKoDNNC/y/AYuCzdL9Jf04rkyRJc8SkLfSquh/49RmIRZIkTdMwLXRJkjTHOTlLT/kzM0laWEbWQk/ytCRfTvK1JN9I8u5WfmCS65JsSvLpJHu28qe29U1t+4pRxSZJUt/ssIWe5H9OsF9V1e9McuzHgGOr6uEkewB/l+QvgbcBH6iqi5J8GDgbOL+9319VByU5A3gvcPpUPowkSQvVRC3074/zKrrE+47JDlydsWFi92ivAo4FLmnl64FT2vKqtk7bflySDP1JJElawCYay/1fJ2RJsjdwLt3P1S5iyMlakuwG3AAcBPwR8G3ggap6vFXZAixty0uBze3cjyd5ENgP+O4UPo8kSQvShJ3ikuxLd4v8tXSt5yPaz9iGUlVPAIcnWQRcChy6E7GOxbQGWAOwfPnynT2cNKPsrChpVHZ4yz3J+4DrgYeAn62q355KMh9UVQ8A1wAvARYlGftDYhmwtS1vBQ5o594deDZw3zjHWldVK6tq5eLFi6cTjiRJvTNRC/3tdB3bfgv47wOPs0P3iPxZEx04yWLgh1X1QJKnA6+g6+h2DXAq3a371cBlbZcNbf0f2/arq6qm86EkjdZcnx1QWogmeoa+sz9p2x9Y356jPwW4uKr+IsktwEVJfhf4CnBBq38B8PEkm4B/Ac7YyfNLkrRgjGxgmaq6CXjhOOW3A0eOU/4o8OpRxSNJUp859KskST1gQpckqQdM6JIk9YAJXZKkHjChS5LUAyZ0SZJ6wIQuSVIPmNAlSeoBE7okST1gQpckqQdGNvSrpNFzkhRJY2yhS5LUAyZ0SZJ6wIQuSVIPmNAlSeoBE7okST1gQpckqQdM6JIk9YAJXZKkHjChS5LUAyZ0SZJ6wIQuSVIPmNAlSeoBE7okST0wsoSe5IAk1yS5Jck3kpzbyvdNcmWS29r7Pq08ST6UZFOSm5IcMarYJEnqm1FOn/o48PaqujHJ3sANSa4E3gBcVVVrk5wHnAe8AzgBOLi9fh44v71rwLDTZUqSFpaRtdCr6q6qurEtPwTcCiwFVgHrW7X1wClteRVwYXWuBRYl2X9U8UmS1Ccz8gw9yQrghcB1wJKquqttuhtY0paXApsHdtvSyiRJ0iRGecsdgCTPBD4DvKWqvpfkX7dVVSWpKR5vDbAGYPny5bsyVKm3hn1Uc8fak0YciaRRGWkLPckedMn8E1X12VZ8z9it9PZ+byvfChwwsPuyVvYkVbWuqlZW1crFixePLnhJkuaRkbXQ0zXFLwBurar3D2zaAKwG1rb3ywbK35TkIrrOcA8O3JqXNAPsdCnNX6O85X40cCZwc5KvtrLfpEvkFyc5G7gTOK1tuwI4EdgEPAKcNcLYJEnqlZEl9Kr6OyA72HzcOPULOGdU8UiS1GeOFCdJUg+Y0CVJ6gETuiRJPWBClySpB0zokiT1gAldkqQeMKFLktQDJnRJknrAhC5JUg+Y0CVJ6gETuiRJPWBClySpB0zokiT1gAldkqQeMKFLktQDJnRJknrAhC5JUg+Y0CVJ6gETuiRJPWBClySpB3af7QD6bsV5l892CJKkBcAWuiRJPWBClySpB0zokiT1gAldkqQeGFmnuCQfBV4J3FtVP9PK9gU+DawA7gBOq6r7kwT4IHAi8Ajwhqq6cVSxSZq/hu1oesfak0YciTS3jLKF/jHg+O3KzgOuqqqDgavaOsAJwMHttQY4f4RxSZLUOyNroVfVF5Os2K54FXBMW14PfAF4Ryu/sKoKuDbJoiT7V9Vdo4pvZ/lzNEnSXDLTz9CXDCTpu4ElbXkpsHmg3pZW9hOSrEmyMcnGbdu2jS5SSZLmkVnrFNda4zWN/dZV1cqqWrl48eIRRCZJ0vwz0yPF3TN2Kz3J/sC9rXwrcMBAvWWtTNI8NpVHU3Zik3bOTLfQNwCr2/Jq4LKB8tencxTw4Fx+fi5J0lwzyp+tfYquA9xzkmwB3gWsBS5OcjZwJ3Baq34F3U/WNtH9bO2sUcUlSVIfjbKX+2t2sOm4ceoWcM6oYpEkqe8cKU6SpB4woUuS1AMmdEmSemCmf7YmSeNy9EVp59hClySpB2yhD7CFIEmar2yhS5LUAyZ0SZJ6wIQuSVIPmNAlSeoBE7okST1gQpckqQdM6JIk9YAJXZKkHjChS5LUAyZ0SZJ6wIQuSVIPOJa7pF4adm6GO9aeNOJIpJlhC12SpB4woUuS1AMmdEmSesBn6JIWNJ+1qy9M6JI0BBO/5jpvuUuS1ANzKqEnOT7Jt5JsSnLebMcjSdJ8MWcSepLdgD8CTgBeALwmyQtmNypJkuaHufQM/UhgU1XdDpDkImAVcMusRiVJU+Czds2WuZTQlwKbB9a3AD8/S7FI0pwx1/9ImM34Zuvcc/G/Sapqxk42kSSnAsdX1Rvb+pnAz1fVm7artwZY01YPAb41xVM9B/juToa7EHndpsfrNj1et+nxuk3PfLpuz62qxeNtmEst9K3AAQPry1rZk1TVOmDddE+SZGNVrZzu/guV1216vG7T43WbHq/b9PTlus2ZTnHA9cDBSQ5MsidwBrBhlmOSJGlemDMt9Kp6PMmbgL8GdgM+WlXfmOWwJEmaF+ZMQgeoqiuAK0Z8mmnfrl/gvG7T43WbHq/b9HjdpqcX123OdIqTJEnTN5eeoUuSpGlaMAndYWWHk+SjSe5N8vWBsn2TXJnktva+z2zGOBclOSDJNUluSfKNJOe2cq/dBJI8LcmXk3ytXbd3t/IDk1zXvq+fbh1ltZ0kuyX5SpK/aOtet0kkuSPJzUm+mmRjK+vF93RBJHSHlZ2SjwHHb1d2HnBVVR0MXNXW9WSPA2+vqhcARwHntP/HvHYTeww4tqoOAw4Hjk9yFPBe4ANVdRBwP3D2LMY4l50L3Dqw7nUbzsur6vCBn6r14nu6IBI6A8PKVtUPgLFhZbWdqvoi8C/bFa8C1rfl9cApMxrUPFBVd1XVjW35Ibp/ZJfitZtQdR5uq3u0VwHHApe0cq/bOJIsA04CPtLWg9dtunrxPV0oCX28YWWXzlIs89GSqrqrLd8NLJnNYOa6JCuAFwLX4bWbVLtt/FXgXuBK4NvAA1X1eKvi93V8fwD8BvCjtr4fXrdhFPD5JDe0kUehJ9/TOfWzNc19VVVJ/GnEDiR5JvAZ4C1V9b2u0dTx2o2vqp4ADk+yCLgUOHSWQ5rzkrwSuLeqbkhyzGzHM8+8tKq2Jvkp4Mok3xzcOJ+/pwulhT7UsLLaoXuS7A/Q3u+d5XjmpCR70CXzT1TVZ1ux125IVfUAcA3wEmBRkrEGh9/Xn3Q0cHKSO+geIR4LfBCv26Sqamt7v5fuD8gj6cn3dKEkdIeV3TkbgNVteTVw2SzGMie155cXALdW1fsHNnntJpBkcWuZk+TpwCvo+h9cA5zaqnndtlNV76yqZVW1gu7fs6ur6rV43SaUZK8ke48tA78EfJ2efE8XzMAySU6ke+Y0Nqzs781ySHNSkk8Bx9DNPnQP8C7gc8DFwHLgTuC0qtq+49yCluSlwJeAm/nxM83fpHuO7rXbgSQ/R9cJaTe6BsbFVfWeJM+ja3nuC3wFeF1VPTZ7kc5d7Zb7f62qV3rdJtauz6VtdXfgk1X1e0n2owff0wWT0CVJ6rOFcstdkqReM6FLktQDJnRJknrAhC5JUg+Y0CVJ6gETujRFSR6evNa4+z01yd+0WZ5O39VxzZQkxyT5hR1se0OSbe0zjr2GnggpyceSnDp5zant32Iem5HsZGdcVB859Ks0c14IUFWHz3YgO+kY4GHgH3aw/dNV9aaZC2dqqmoDDiylHrKFLu1ibW7lzyW5Kcm1SX6ujRv9p8CLW6v1+dvtc1BrvX8tyY1Jnp/O+5J8vc3ffHqre0ySv01yWZLbk6xN8tp084rfPHbs1lo9v8Vwe9vvo0luTfKxgXP/UpJ/bOf9szYe/di80e9u5TcnObRNPPMrwFvb5/j3Q16ToWJufjHJxiT/1MYsH5vA5X1Jrm/X9ZdbeZL8YZJvJfkb4KcGznl8km8muRF41UD5G5L84cA1+lCSf2hxndrKn5Lkj9v+Vya5YmDb2nTz3t+U5PeH+fzSTLCFLu167wa+UlWnJDkWuLCqDk/yRtqIXuPs8wlgbVVdmuRpdH9sv4pujvDD6Ebuuz7JF1v9w4B/SzfV7e3AR6rqyCTnAm8G3tLq7UM3NvrJdK3So4E3tmMdTjcj128Bv1hV30/yDuBtwHva/t+tqiOS/FqL/Y1JPgw8XFU7Smanpxs5b8xLphjzCrrxtZ8PXJPkIOD1wINV9eIkTwX+Psnn6e56HAK8gG6GrFuAj7Zr+L/pxjjfBHx6B7EC7A+8lG5SmA1004++qsXxAro/Em5tx90P+E/AoW0Sj0UTHFeaUbbQpV3vpcDHAarqamC/JM/aUeV0Y0svrapL2z6PVtUj7Tifqqonquoe4G+BF7fdrm9zsD9GN93o51v5zXSJaMyfVzcc5M3APVV1c1X9CPhGq3cUXdL6+3RTmK4Gnjuw/9gkMzdsd9yJfLqqDh94/b8pxnxxVf2oqm6jS/yH0o25/foW43V0U4UeDLxs4Br9X+DqdoxDgX+uqtva5//TCeL9XDvfLfx42syXAn/Wyu+mGyMd4EHgUeCCJK8CHhnymkgjZwtdmp8Gx+f+0cD6j3jy9/qxceoM1nsCuLKqXjPJeZ5g5/+9GDbm7cejLiDAm6vqrwc3pJujYWcNxpUd1gKq6vEkRwLH0U2C8ia6uwDSrLOFLu16XwJeC/86ccZ3q+p7O6pcVQ8BW5Kc0vZ5apJntOOc3p4fL6ZrjX55F8d6LXB0u609NhvVT0+yz0PA3rs4jkGvbs+wnw88D/gW8NfAr6abopYkP51utqwv8uNrtD/w8naMbwIrBp7N7+gPlh35e+A/tziW0HUEHJvv/tlVdQXwVrrHCNKcYAtdmrpnJNkysP7+7aZM/W2656030d2SXc3kzgT+JMl7gB8Cr6abFeolwNfoWqm/UVV3Jzl0F3wGAKpqW5I3AJ9qz6ahe6b+TxPs9ufAJUlW0bWav7Td9u2fof/aFMP6Dt0fLs8CfqWqHk3yEbrb8jcmCbANOIXuGh1L9+z8O8A/ts/1aJI1wOVJHqH742gqf4R8hq4VfguwGbiR7nb73sBl7Rl96PobSHOCs61J0jiSPLOqHm4d4b4MHN2ep0tzki10SRrfX7Re7HsCv2My11xnC12SpB6wU5wkST1gQpckqQdM6JIk9YAJXZKkHjChS5LUAyZ0SZJ64P8Dug6Lkne9QVMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_sentence_embeddings_length(text_list, tokenizer):\n",
    "    tokenized_texts = list(map(lambda t: tokenizer.tokenize(t), text_list))\n",
    "    tokenized_text_len = list(map(lambda t: len(t), tokenized_texts))\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.hist(tokenized_text_len, bins=40)\n",
    "    ax.set_xlabel(\"L of comment Embeddings\")\n",
    "    ax.set_ylabel(\"N of Comments\")\n",
    "    return\n",
    "\n",
    "plot_sentence_embeddings_length(texts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/herokuma/miniconda3/envs/pytorch/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "indices = tokenizer.batch_encode_plus(texts, max_length=64, add_special_tokens=True, return_attention_mask=True, pad_to_max_length=True, truncation=True)\n",
    "input_ids = indices[\"input_ids\"]\n",
    "attention_masks = indices[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use 99% for training and 1% for validation.\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=42, test_size=0.2)\n",
    "# Do the same for the masks.\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
    "                                             random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "validation_labels = torch.tensor(validation_labels, dtype=torch.long)\n",
    "train_masks = torch.tensor(train_masks, dtype=torch.long)\n",
    "validation_masks = torch.tensor(validation_masks, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                 lr = 6e-6,\n",
    "                 eps = 1e-8\n",
    "                 )\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 5\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                           num_warmup_steps = 0,\n",
    "                                           num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch    50  of  6,048.    Elapsed: 0:00:03.\n",
      "  Batch   100  of  6,048.    Elapsed: 0:00:06.\n",
      "  Batch   150  of  6,048.    Elapsed: 0:00:09.\n",
      "  Batch   200  of  6,048.    Elapsed: 0:00:11.\n",
      "  Batch   250  of  6,048.    Elapsed: 0:00:14.\n",
      "  Batch   300  of  6,048.    Elapsed: 0:00:17.\n",
      "  Batch   350  of  6,048.    Elapsed: 0:00:20.\n",
      "  Batch   400  of  6,048.    Elapsed: 0:00:23.\n",
      "  Batch   450  of  6,048.    Elapsed: 0:00:26.\n",
      "  Batch   500  of  6,048.    Elapsed: 0:00:28.\n",
      "  Batch   550  of  6,048.    Elapsed: 0:00:31.\n",
      "  Batch   600  of  6,048.    Elapsed: 0:00:34.\n",
      "  Batch   650  of  6,048.    Elapsed: 0:00:37.\n",
      "  Batch   700  of  6,048.    Elapsed: 0:00:39.\n",
      "  Batch   750  of  6,048.    Elapsed: 0:00:42.\n",
      "  Batch   800  of  6,048.    Elapsed: 0:00:45.\n",
      "  Batch   850  of  6,048.    Elapsed: 0:00:48.\n",
      "  Batch   900  of  6,048.    Elapsed: 0:00:50.\n",
      "  Batch   950  of  6,048.    Elapsed: 0:00:53.\n",
      "  Batch 1,000  of  6,048.    Elapsed: 0:00:56.\n",
      "  Batch 1,050  of  6,048.    Elapsed: 0:00:59.\n",
      "  Batch 1,100  of  6,048.    Elapsed: 0:01:02.\n",
      "  Batch 1,150  of  6,048.    Elapsed: 0:01:04.\n",
      "  Batch 1,200  of  6,048.    Elapsed: 0:01:07.\n",
      "  Batch 1,250  of  6,048.    Elapsed: 0:01:10.\n",
      "  Batch 1,300  of  6,048.    Elapsed: 0:01:13.\n",
      "  Batch 1,350  of  6,048.    Elapsed: 0:01:15.\n",
      "  Batch 1,400  of  6,048.    Elapsed: 0:01:18.\n",
      "  Batch 1,450  of  6,048.    Elapsed: 0:01:21.\n",
      "  Batch 1,500  of  6,048.    Elapsed: 0:01:23.\n",
      "  Batch 1,550  of  6,048.    Elapsed: 0:01:26.\n",
      "  Batch 1,600  of  6,048.    Elapsed: 0:01:29.\n",
      "  Batch 1,650  of  6,048.    Elapsed: 0:01:32.\n",
      "  Batch 1,700  of  6,048.    Elapsed: 0:01:34.\n",
      "  Batch 1,750  of  6,048.    Elapsed: 0:01:37.\n",
      "  Batch 1,800  of  6,048.    Elapsed: 0:01:40.\n",
      "  Batch 1,850  of  6,048.    Elapsed: 0:01:42.\n",
      "  Batch 1,900  of  6,048.    Elapsed: 0:01:45.\n",
      "  Batch 1,950  of  6,048.    Elapsed: 0:01:48.\n",
      "  Batch 2,000  of  6,048.    Elapsed: 0:01:51.\n",
      "  Batch 2,050  of  6,048.    Elapsed: 0:01:53.\n",
      "  Batch 2,100  of  6,048.    Elapsed: 0:01:56.\n",
      "  Batch 2,150  of  6,048.    Elapsed: 0:01:59.\n",
      "  Batch 2,200  of  6,048.    Elapsed: 0:02:01.\n",
      "  Batch 2,250  of  6,048.    Elapsed: 0:02:04.\n",
      "  Batch 2,300  of  6,048.    Elapsed: 0:02:07.\n",
      "  Batch 2,350  of  6,048.    Elapsed: 0:02:10.\n",
      "  Batch 2,400  of  6,048.    Elapsed: 0:02:12.\n",
      "  Batch 2,450  of  6,048.    Elapsed: 0:02:15.\n",
      "  Batch 2,500  of  6,048.    Elapsed: 0:02:18.\n",
      "  Batch 2,550  of  6,048.    Elapsed: 0:02:21.\n",
      "  Batch 2,600  of  6,048.    Elapsed: 0:02:23.\n",
      "  Batch 2,650  of  6,048.    Elapsed: 0:02:26.\n",
      "  Batch 2,700  of  6,048.    Elapsed: 0:02:29.\n",
      "  Batch 2,750  of  6,048.    Elapsed: 0:02:31.\n",
      "  Batch 2,800  of  6,048.    Elapsed: 0:02:34.\n",
      "  Batch 2,850  of  6,048.    Elapsed: 0:02:37.\n",
      "  Batch 2,900  of  6,048.    Elapsed: 0:02:40.\n",
      "  Batch 2,950  of  6,048.    Elapsed: 0:02:42.\n",
      "  Batch 3,000  of  6,048.    Elapsed: 0:02:45.\n",
      "  Batch 3,050  of  6,048.    Elapsed: 0:02:48.\n",
      "  Batch 3,100  of  6,048.    Elapsed: 0:02:50.\n",
      "  Batch 3,150  of  6,048.    Elapsed: 0:02:53.\n",
      "  Batch 3,200  of  6,048.    Elapsed: 0:02:56.\n",
      "  Batch 3,250  of  6,048.    Elapsed: 0:02:58.\n",
      "  Batch 3,300  of  6,048.    Elapsed: 0:03:01.\n",
      "  Batch 3,350  of  6,048.    Elapsed: 0:03:04.\n",
      "  Batch 3,400  of  6,048.    Elapsed: 0:03:07.\n",
      "  Batch 3,450  of  6,048.    Elapsed: 0:03:09.\n",
      "  Batch 3,500  of  6,048.    Elapsed: 0:03:12.\n",
      "  Batch 3,550  of  6,048.    Elapsed: 0:03:15.\n",
      "  Batch 3,600  of  6,048.    Elapsed: 0:03:17.\n",
      "  Batch 3,650  of  6,048.    Elapsed: 0:03:20.\n",
      "  Batch 3,700  of  6,048.    Elapsed: 0:03:23.\n",
      "  Batch 3,750  of  6,048.    Elapsed: 0:03:26.\n",
      "  Batch 3,800  of  6,048.    Elapsed: 0:03:28.\n",
      "  Batch 3,850  of  6,048.    Elapsed: 0:03:31.\n",
      "  Batch 3,900  of  6,048.    Elapsed: 0:03:34.\n",
      "  Batch 3,950  of  6,048.    Elapsed: 0:03:36.\n",
      "  Batch 4,000  of  6,048.    Elapsed: 0:03:39.\n",
      "  Batch 4,050  of  6,048.    Elapsed: 0:03:42.\n",
      "  Batch 4,100  of  6,048.    Elapsed: 0:03:45.\n",
      "  Batch 4,150  of  6,048.    Elapsed: 0:03:47.\n",
      "  Batch 4,200  of  6,048.    Elapsed: 0:03:50.\n",
      "  Batch 4,250  of  6,048.    Elapsed: 0:03:53.\n",
      "  Batch 4,300  of  6,048.    Elapsed: 0:03:55.\n",
      "  Batch 4,350  of  6,048.    Elapsed: 0:03:58.\n",
      "  Batch 4,400  of  6,048.    Elapsed: 0:04:01.\n",
      "  Batch 4,450  of  6,048.    Elapsed: 0:04:04.\n",
      "  Batch 4,500  of  6,048.    Elapsed: 0:04:06.\n",
      "  Batch 4,550  of  6,048.    Elapsed: 0:04:09.\n",
      "  Batch 4,600  of  6,048.    Elapsed: 0:04:12.\n",
      "  Batch 4,650  of  6,048.    Elapsed: 0:04:15.\n",
      "  Batch 4,700  of  6,048.    Elapsed: 0:04:17.\n",
      "  Batch 4,750  of  6,048.    Elapsed: 0:04:20.\n",
      "  Batch 4,800  of  6,048.    Elapsed: 0:04:23.\n",
      "  Batch 4,850  of  6,048.    Elapsed: 0:04:26.\n",
      "  Batch 4,900  of  6,048.    Elapsed: 0:04:28.\n",
      "  Batch 4,950  of  6,048.    Elapsed: 0:04:31.\n",
      "  Batch 5,000  of  6,048.    Elapsed: 0:04:34.\n",
      "  Batch 5,050  of  6,048.    Elapsed: 0:04:37.\n",
      "  Batch 5,100  of  6,048.    Elapsed: 0:04:39.\n",
      "  Batch 5,150  of  6,048.    Elapsed: 0:04:42.\n",
      "  Batch 5,200  of  6,048.    Elapsed: 0:04:45.\n",
      "  Batch 5,250  of  6,048.    Elapsed: 0:04:48.\n",
      "  Batch 5,300  of  6,048.    Elapsed: 0:04:50.\n",
      "  Batch 5,350  of  6,048.    Elapsed: 0:04:53.\n",
      "  Batch 5,400  of  6,048.    Elapsed: 0:04:56.\n",
      "  Batch 5,450  of  6,048.    Elapsed: 0:04:59.\n",
      "  Batch 5,500  of  6,048.    Elapsed: 0:05:01.\n",
      "  Batch 5,550  of  6,048.    Elapsed: 0:05:04.\n",
      "  Batch 5,600  of  6,048.    Elapsed: 0:05:07.\n",
      "  Batch 5,650  of  6,048.    Elapsed: 0:05:10.\n",
      "  Batch 5,700  of  6,048.    Elapsed: 0:05:12.\n",
      "  Batch 5,750  of  6,048.    Elapsed: 0:05:15.\n",
      "  Batch 5,800  of  6,048.    Elapsed: 0:05:18.\n",
      "  Batch 5,850  of  6,048.    Elapsed: 0:05:21.\n",
      "  Batch 5,900  of  6,048.    Elapsed: 0:05:23.\n",
      "  Batch 5,950  of  6,048.    Elapsed: 0:05:26.\n",
      "  Batch 6,000  of  6,048.    Elapsed: 0:05:29.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoch took: 0:05:31\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    50  of  6,048.    Elapsed: 0:00:03.\n",
      "  Batch   100  of  6,048.    Elapsed: 0:00:05.\n",
      "  Batch   150  of  6,048.    Elapsed: 0:00:08.\n",
      "  Batch   200  of  6,048.    Elapsed: 0:00:11.\n",
      "  Batch   250  of  6,048.    Elapsed: 0:00:14.\n",
      "  Batch   300  of  6,048.    Elapsed: 0:00:16.\n",
      "  Batch   350  of  6,048.    Elapsed: 0:00:19.\n",
      "  Batch   400  of  6,048.    Elapsed: 0:00:22.\n",
      "  Batch   450  of  6,048.    Elapsed: 0:00:25.\n",
      "  Batch   500  of  6,048.    Elapsed: 0:00:27.\n",
      "  Batch   550  of  6,048.    Elapsed: 0:00:30.\n",
      "  Batch   600  of  6,048.    Elapsed: 0:00:33.\n",
      "  Batch   650  of  6,048.    Elapsed: 0:00:36.\n",
      "  Batch   700  of  6,048.    Elapsed: 0:00:38.\n",
      "  Batch   750  of  6,048.    Elapsed: 0:00:41.\n",
      "  Batch   800  of  6,048.    Elapsed: 0:00:44.\n",
      "  Batch   850  of  6,048.    Elapsed: 0:00:46.\n",
      "  Batch   900  of  6,048.    Elapsed: 0:00:49.\n",
      "  Batch   950  of  6,048.    Elapsed: 0:00:52.\n",
      "  Batch 1,000  of  6,048.    Elapsed: 0:00:55.\n",
      "  Batch 1,050  of  6,048.    Elapsed: 0:00:57.\n",
      "  Batch 1,100  of  6,048.    Elapsed: 0:01:00.\n",
      "  Batch 1,150  of  6,048.    Elapsed: 0:01:03.\n",
      "  Batch 1,200  of  6,048.    Elapsed: 0:01:06.\n",
      "  Batch 1,250  of  6,048.    Elapsed: 0:01:08.\n",
      "  Batch 1,300  of  6,048.    Elapsed: 0:01:11.\n",
      "  Batch 1,350  of  6,048.    Elapsed: 0:01:14.\n",
      "  Batch 1,400  of  6,048.    Elapsed: 0:01:17.\n",
      "  Batch 1,450  of  6,048.    Elapsed: 0:01:19.\n",
      "  Batch 1,500  of  6,048.    Elapsed: 0:01:22.\n",
      "  Batch 1,550  of  6,048.    Elapsed: 0:01:25.\n",
      "  Batch 1,600  of  6,048.    Elapsed: 0:01:27.\n",
      "  Batch 1,650  of  6,048.    Elapsed: 0:01:30.\n",
      "  Batch 1,700  of  6,048.    Elapsed: 0:01:33.\n",
      "  Batch 1,750  of  6,048.    Elapsed: 0:01:36.\n",
      "  Batch 1,800  of  6,048.    Elapsed: 0:01:38.\n",
      "  Batch 1,850  of  6,048.    Elapsed: 0:01:41.\n",
      "  Batch 1,900  of  6,048.    Elapsed: 0:01:44.\n",
      "  Batch 1,950  of  6,048.    Elapsed: 0:01:47.\n",
      "  Batch 2,000  of  6,048.    Elapsed: 0:01:49.\n",
      "  Batch 2,050  of  6,048.    Elapsed: 0:01:52.\n",
      "  Batch 2,100  of  6,048.    Elapsed: 0:01:55.\n",
      "  Batch 2,150  of  6,048.    Elapsed: 0:01:58.\n",
      "  Batch 2,200  of  6,048.    Elapsed: 0:02:00.\n",
      "  Batch 2,250  of  6,048.    Elapsed: 0:02:03.\n",
      "  Batch 2,300  of  6,048.    Elapsed: 0:02:06.\n",
      "  Batch 2,350  of  6,048.    Elapsed: 0:02:08.\n",
      "  Batch 2,400  of  6,048.    Elapsed: 0:02:11.\n",
      "  Batch 2,450  of  6,048.    Elapsed: 0:02:14.\n",
      "  Batch 2,500  of  6,048.    Elapsed: 0:02:17.\n",
      "  Batch 2,550  of  6,048.    Elapsed: 0:02:19.\n",
      "  Batch 2,600  of  6,048.    Elapsed: 0:02:22.\n",
      "  Batch 2,650  of  6,048.    Elapsed: 0:02:25.\n",
      "  Batch 2,700  of  6,048.    Elapsed: 0:02:28.\n",
      "  Batch 2,750  of  6,048.    Elapsed: 0:02:30.\n",
      "  Batch 2,800  of  6,048.    Elapsed: 0:02:33.\n",
      "  Batch 2,850  of  6,048.    Elapsed: 0:02:36.\n",
      "  Batch 2,900  of  6,048.    Elapsed: 0:02:38.\n",
      "  Batch 2,950  of  6,048.    Elapsed: 0:02:41.\n",
      "  Batch 3,000  of  6,048.    Elapsed: 0:02:44.\n",
      "  Batch 3,050  of  6,048.    Elapsed: 0:02:47.\n",
      "  Batch 3,100  of  6,048.    Elapsed: 0:02:49.\n",
      "  Batch 3,150  of  6,048.    Elapsed: 0:02:52.\n",
      "  Batch 3,200  of  6,048.    Elapsed: 0:02:55.\n",
      "  Batch 3,250  of  6,048.    Elapsed: 0:02:58.\n",
      "  Batch 3,300  of  6,048.    Elapsed: 0:03:00.\n",
      "  Batch 3,350  of  6,048.    Elapsed: 0:03:03.\n",
      "  Batch 3,400  of  6,048.    Elapsed: 0:03:06.\n",
      "  Batch 3,450  of  6,048.    Elapsed: 0:03:09.\n",
      "  Batch 3,500  of  6,048.    Elapsed: 0:03:11.\n",
      "  Batch 3,550  of  6,048.    Elapsed: 0:03:14.\n",
      "  Batch 3,600  of  6,048.    Elapsed: 0:03:17.\n",
      "  Batch 3,650  of  6,048.    Elapsed: 0:03:20.\n",
      "  Batch 3,700  of  6,048.    Elapsed: 0:03:22.\n",
      "  Batch 3,750  of  6,048.    Elapsed: 0:03:25.\n",
      "  Batch 3,800  of  6,048.    Elapsed: 0:03:28.\n",
      "  Batch 3,850  of  6,048.    Elapsed: 0:03:30.\n",
      "  Batch 3,900  of  6,048.    Elapsed: 0:03:33.\n",
      "  Batch 3,950  of  6,048.    Elapsed: 0:03:36.\n",
      "  Batch 4,000  of  6,048.    Elapsed: 0:03:39.\n",
      "  Batch 4,050  of  6,048.    Elapsed: 0:03:41.\n",
      "  Batch 4,100  of  6,048.    Elapsed: 0:03:44.\n",
      "  Batch 4,150  of  6,048.    Elapsed: 0:03:47.\n",
      "  Batch 4,200  of  6,048.    Elapsed: 0:03:50.\n",
      "  Batch 4,250  of  6,048.    Elapsed: 0:03:52.\n",
      "  Batch 4,300  of  6,048.    Elapsed: 0:03:55.\n",
      "  Batch 4,350  of  6,048.    Elapsed: 0:03:58.\n",
      "  Batch 4,400  of  6,048.    Elapsed: 0:04:01.\n",
      "  Batch 4,450  of  6,048.    Elapsed: 0:04:03.\n",
      "  Batch 4,500  of  6,048.    Elapsed: 0:04:06.\n",
      "  Batch 4,550  of  6,048.    Elapsed: 0:04:09.\n",
      "  Batch 4,600  of  6,048.    Elapsed: 0:04:11.\n",
      "  Batch 4,650  of  6,048.    Elapsed: 0:04:14.\n",
      "  Batch 4,700  of  6,048.    Elapsed: 0:04:17.\n",
      "  Batch 4,750  of  6,048.    Elapsed: 0:04:19.\n",
      "  Batch 4,800  of  6,048.    Elapsed: 0:04:22.\n",
      "  Batch 4,850  of  6,048.    Elapsed: 0:04:25.\n",
      "  Batch 4,900  of  6,048.    Elapsed: 0:04:27.\n",
      "  Batch 4,950  of  6,048.    Elapsed: 0:04:30.\n",
      "  Batch 5,000  of  6,048.    Elapsed: 0:04:33.\n",
      "  Batch 5,050  of  6,048.    Elapsed: 0:04:35.\n",
      "  Batch 5,100  of  6,048.    Elapsed: 0:04:38.\n",
      "  Batch 5,150  of  6,048.    Elapsed: 0:04:41.\n",
      "  Batch 5,200  of  6,048.    Elapsed: 0:04:43.\n",
      "  Batch 5,250  of  6,048.    Elapsed: 0:04:46.\n",
      "  Batch 5,300  of  6,048.    Elapsed: 0:04:48.\n",
      "  Batch 5,350  of  6,048.    Elapsed: 0:04:51.\n",
      "  Batch 5,400  of  6,048.    Elapsed: 0:04:54.\n",
      "  Batch 5,450  of  6,048.    Elapsed: 0:04:56.\n",
      "  Batch 5,500  of  6,048.    Elapsed: 0:04:59.\n",
      "  Batch 5,550  of  6,048.    Elapsed: 0:05:02.\n",
      "  Batch 5,600  of  6,048.    Elapsed: 0:05:04.\n",
      "  Batch 5,650  of  6,048.    Elapsed: 0:05:07.\n",
      "  Batch 5,700  of  6,048.    Elapsed: 0:05:10.\n",
      "  Batch 5,750  of  6,048.    Elapsed: 0:05:13.\n",
      "  Batch 5,800  of  6,048.    Elapsed: 0:05:15.\n",
      "  Batch 5,850  of  6,048.    Elapsed: 0:05:18.\n",
      "  Batch 5,900  of  6,048.    Elapsed: 0:05:21.\n",
      "  Batch 5,950  of  6,048.    Elapsed: 0:05:24.\n",
      "  Batch 6,000  of  6,048.    Elapsed: 0:05:26.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoch took: 0:05:29\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    50  of  6,048.    Elapsed: 0:00:03.\n",
      "  Batch   100  of  6,048.    Elapsed: 0:00:05.\n",
      "  Batch   150  of  6,048.    Elapsed: 0:00:08.\n",
      "  Batch   200  of  6,048.    Elapsed: 0:00:11.\n",
      "  Batch   250  of  6,048.    Elapsed: 0:00:14.\n",
      "  Batch   300  of  6,048.    Elapsed: 0:00:16.\n",
      "  Batch   350  of  6,048.    Elapsed: 0:00:19.\n",
      "  Batch   400  of  6,048.    Elapsed: 0:00:22.\n",
      "  Batch   450  of  6,048.    Elapsed: 0:00:25.\n",
      "  Batch   500  of  6,048.    Elapsed: 0:00:27.\n",
      "  Batch   550  of  6,048.    Elapsed: 0:00:30.\n",
      "  Batch   600  of  6,048.    Elapsed: 0:00:33.\n",
      "  Batch   650  of  6,048.    Elapsed: 0:00:36.\n",
      "  Batch   700  of  6,048.    Elapsed: 0:00:38.\n",
      "  Batch   750  of  6,048.    Elapsed: 0:00:41.\n",
      "  Batch   800  of  6,048.    Elapsed: 0:00:44.\n",
      "  Batch   850  of  6,048.    Elapsed: 0:00:47.\n",
      "  Batch   900  of  6,048.    Elapsed: 0:00:49.\n",
      "  Batch   950  of  6,048.    Elapsed: 0:00:52.\n",
      "  Batch 1,000  of  6,048.    Elapsed: 0:00:55.\n",
      "  Batch 1,050  of  6,048.    Elapsed: 0:00:58.\n",
      "  Batch 1,100  of  6,048.    Elapsed: 0:01:00.\n",
      "  Batch 1,150  of  6,048.    Elapsed: 0:01:03.\n",
      "  Batch 1,200  of  6,048.    Elapsed: 0:01:06.\n",
      "  Batch 1,250  of  6,048.    Elapsed: 0:01:09.\n",
      "  Batch 1,300  of  6,048.    Elapsed: 0:01:11.\n",
      "  Batch 1,350  of  6,048.    Elapsed: 0:01:14.\n",
      "  Batch 1,400  of  6,048.    Elapsed: 0:01:17.\n",
      "  Batch 1,450  of  6,048.    Elapsed: 0:01:19.\n",
      "  Batch 1,500  of  6,048.    Elapsed: 0:01:22.\n",
      "  Batch 1,550  of  6,048.    Elapsed: 0:01:25.\n",
      "  Batch 1,600  of  6,048.    Elapsed: 0:01:27.\n",
      "  Batch 1,650  of  6,048.    Elapsed: 0:01:30.\n",
      "  Batch 1,700  of  6,048.    Elapsed: 0:01:33.\n",
      "  Batch 1,750  of  6,048.    Elapsed: 0:01:35.\n",
      "  Batch 1,800  of  6,048.    Elapsed: 0:01:38.\n",
      "  Batch 1,850  of  6,048.    Elapsed: 0:01:40.\n",
      "  Batch 1,900  of  6,048.    Elapsed: 0:01:43.\n",
      "  Batch 1,950  of  6,048.    Elapsed: 0:01:46.\n",
      "  Batch 2,000  of  6,048.    Elapsed: 0:01:48.\n",
      "  Batch 2,050  of  6,048.    Elapsed: 0:01:51.\n",
      "  Batch 2,100  of  6,048.    Elapsed: 0:01:53.\n",
      "  Batch 2,150  of  6,048.    Elapsed: 0:01:56.\n",
      "  Batch 2,200  of  6,048.    Elapsed: 0:01:59.\n",
      "  Batch 2,250  of  6,048.    Elapsed: 0:02:01.\n",
      "  Batch 2,300  of  6,048.    Elapsed: 0:02:04.\n",
      "  Batch 2,350  of  6,048.    Elapsed: 0:02:07.\n",
      "  Batch 2,400  of  6,048.    Elapsed: 0:02:09.\n",
      "  Batch 2,450  of  6,048.    Elapsed: 0:02:12.\n",
      "  Batch 2,500  of  6,048.    Elapsed: 0:02:14.\n",
      "  Batch 2,550  of  6,048.    Elapsed: 0:02:17.\n",
      "  Batch 2,600  of  6,048.    Elapsed: 0:02:20.\n",
      "  Batch 2,650  of  6,048.    Elapsed: 0:02:22.\n",
      "  Batch 2,700  of  6,048.    Elapsed: 0:02:25.\n",
      "  Batch 2,750  of  6,048.    Elapsed: 0:02:27.\n",
      "  Batch 2,800  of  6,048.    Elapsed: 0:02:30.\n",
      "  Batch 2,850  of  6,048.    Elapsed: 0:02:33.\n",
      "  Batch 2,900  of  6,048.    Elapsed: 0:02:35.\n",
      "  Batch 2,950  of  6,048.    Elapsed: 0:02:38.\n",
      "  Batch 3,000  of  6,048.    Elapsed: 0:02:41.\n",
      "  Batch 3,050  of  6,048.    Elapsed: 0:02:43.\n",
      "  Batch 3,100  of  6,048.    Elapsed: 0:02:46.\n",
      "  Batch 3,150  of  6,048.    Elapsed: 0:02:49.\n",
      "  Batch 3,200  of  6,048.    Elapsed: 0:02:51.\n",
      "  Batch 3,250  of  6,048.    Elapsed: 0:02:54.\n",
      "  Batch 3,300  of  6,048.    Elapsed: 0:02:57.\n",
      "  Batch 3,350  of  6,048.    Elapsed: 0:03:00.\n",
      "  Batch 3,400  of  6,048.    Elapsed: 0:03:02.\n",
      "  Batch 3,450  of  6,048.    Elapsed: 0:03:05.\n",
      "  Batch 3,500  of  6,048.    Elapsed: 0:03:08.\n",
      "  Batch 3,550  of  6,048.    Elapsed: 0:03:11.\n",
      "  Batch 3,600  of  6,048.    Elapsed: 0:03:13.\n",
      "  Batch 3,650  of  6,048.    Elapsed: 0:03:16.\n",
      "  Batch 3,700  of  6,048.    Elapsed: 0:03:19.\n",
      "  Batch 3,750  of  6,048.    Elapsed: 0:03:22.\n",
      "  Batch 3,800  of  6,048.    Elapsed: 0:03:24.\n",
      "  Batch 3,850  of  6,048.    Elapsed: 0:03:27.\n",
      "  Batch 3,900  of  6,048.    Elapsed: 0:03:30.\n",
      "  Batch 3,950  of  6,048.    Elapsed: 0:03:32.\n",
      "  Batch 4,000  of  6,048.    Elapsed: 0:03:35.\n",
      "  Batch 4,050  of  6,048.    Elapsed: 0:03:38.\n",
      "  Batch 4,100  of  6,048.    Elapsed: 0:03:41.\n",
      "  Batch 4,150  of  6,048.    Elapsed: 0:03:43.\n",
      "  Batch 4,200  of  6,048.    Elapsed: 0:03:46.\n",
      "  Batch 4,250  of  6,048.    Elapsed: 0:03:49.\n",
      "  Batch 4,300  of  6,048.    Elapsed: 0:03:52.\n",
      "  Batch 4,350  of  6,048.    Elapsed: 0:03:54.\n",
      "  Batch 4,400  of  6,048.    Elapsed: 0:03:57.\n",
      "  Batch 4,450  of  6,048.    Elapsed: 0:04:00.\n",
      "  Batch 4,500  of  6,048.    Elapsed: 0:04:02.\n",
      "  Batch 4,550  of  6,048.    Elapsed: 0:04:05.\n",
      "  Batch 4,600  of  6,048.    Elapsed: 0:04:08.\n",
      "  Batch 4,650  of  6,048.    Elapsed: 0:04:10.\n",
      "  Batch 4,700  of  6,048.    Elapsed: 0:04:13.\n",
      "  Batch 4,750  of  6,048.    Elapsed: 0:04:15.\n",
      "  Batch 4,800  of  6,048.    Elapsed: 0:04:18.\n",
      "  Batch 4,850  of  6,048.    Elapsed: 0:04:21.\n",
      "  Batch 4,900  of  6,048.    Elapsed: 0:04:23.\n",
      "  Batch 4,950  of  6,048.    Elapsed: 0:04:26.\n",
      "  Batch 5,000  of  6,048.    Elapsed: 0:04:29.\n",
      "  Batch 5,050  of  6,048.    Elapsed: 0:04:31.\n",
      "  Batch 5,100  of  6,048.    Elapsed: 0:04:34.\n",
      "  Batch 5,150  of  6,048.    Elapsed: 0:04:36.\n",
      "  Batch 5,200  of  6,048.    Elapsed: 0:04:39.\n",
      "  Batch 5,250  of  6,048.    Elapsed: 0:04:42.\n",
      "  Batch 5,300  of  6,048.    Elapsed: 0:04:44.\n",
      "  Batch 5,350  of  6,048.    Elapsed: 0:04:47.\n",
      "  Batch 5,400  of  6,048.    Elapsed: 0:04:50.\n",
      "  Batch 5,450  of  6,048.    Elapsed: 0:04:52.\n",
      "  Batch 5,500  of  6,048.    Elapsed: 0:04:55.\n",
      "  Batch 5,550  of  6,048.    Elapsed: 0:04:58.\n",
      "  Batch 5,600  of  6,048.    Elapsed: 0:05:01.\n",
      "  Batch 5,650  of  6,048.    Elapsed: 0:05:03.\n",
      "  Batch 5,700  of  6,048.    Elapsed: 0:05:06.\n",
      "  Batch 5,750  of  6,048.    Elapsed: 0:05:08.\n",
      "  Batch 5,800  of  6,048.    Elapsed: 0:05:11.\n",
      "  Batch 5,850  of  6,048.    Elapsed: 0:05:14.\n",
      "  Batch 5,900  of  6,048.    Elapsed: 0:05:17.\n",
      "  Batch 5,950  of  6,048.    Elapsed: 0:05:19.\n",
      "  Batch 6,000  of  6,048.    Elapsed: 0:05:22.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoch took: 0:05:25\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    50  of  6,048.    Elapsed: 0:00:03.\n",
      "  Batch   100  of  6,048.    Elapsed: 0:00:05.\n",
      "  Batch   150  of  6,048.    Elapsed: 0:00:08.\n",
      "  Batch   200  of  6,048.    Elapsed: 0:00:11.\n",
      "  Batch   250  of  6,048.    Elapsed: 0:00:14.\n",
      "  Batch   300  of  6,048.    Elapsed: 0:00:16.\n",
      "  Batch   350  of  6,048.    Elapsed: 0:00:19.\n",
      "  Batch   400  of  6,048.    Elapsed: 0:00:22.\n",
      "  Batch   450  of  6,048.    Elapsed: 0:00:25.\n",
      "  Batch   500  of  6,048.    Elapsed: 0:00:27.\n",
      "  Batch   550  of  6,048.    Elapsed: 0:00:30.\n",
      "  Batch   600  of  6,048.    Elapsed: 0:00:33.\n",
      "  Batch   650  of  6,048.    Elapsed: 0:00:35.\n",
      "  Batch   700  of  6,048.    Elapsed: 0:00:38.\n",
      "  Batch   750  of  6,048.    Elapsed: 0:00:41.\n",
      "  Batch   800  of  6,048.    Elapsed: 0:00:44.\n",
      "  Batch   850  of  6,048.    Elapsed: 0:00:46.\n",
      "  Batch   900  of  6,048.    Elapsed: 0:00:49.\n",
      "  Batch   950  of  6,048.    Elapsed: 0:00:52.\n",
      "  Batch 1,000  of  6,048.    Elapsed: 0:00:55.\n",
      "  Batch 1,050  of  6,048.    Elapsed: 0:00:57.\n",
      "  Batch 1,100  of  6,048.    Elapsed: 0:01:00.\n",
      "  Batch 1,150  of  6,048.    Elapsed: 0:01:03.\n",
      "  Batch 1,200  of  6,048.    Elapsed: 0:01:05.\n",
      "  Batch 1,250  of  6,048.    Elapsed: 0:01:08.\n",
      "  Batch 1,300  of  6,048.    Elapsed: 0:01:11.\n",
      "  Batch 1,350  of  6,048.    Elapsed: 0:01:14.\n",
      "  Batch 1,400  of  6,048.    Elapsed: 0:01:16.\n",
      "  Batch 1,450  of  6,048.    Elapsed: 0:01:19.\n",
      "  Batch 1,500  of  6,048.    Elapsed: 0:01:22.\n",
      "  Batch 1,550  of  6,048.    Elapsed: 0:01:25.\n",
      "  Batch 1,600  of  6,048.    Elapsed: 0:01:27.\n",
      "  Batch 1,650  of  6,048.    Elapsed: 0:01:30.\n",
      "  Batch 1,700  of  6,048.    Elapsed: 0:01:33.\n",
      "  Batch 1,750  of  6,048.    Elapsed: 0:01:35.\n",
      "  Batch 1,800  of  6,048.    Elapsed: 0:01:38.\n",
      "  Batch 1,850  of  6,048.    Elapsed: 0:01:41.\n",
      "  Batch 1,900  of  6,048.    Elapsed: 0:01:44.\n",
      "  Batch 1,950  of  6,048.    Elapsed: 0:01:46.\n",
      "  Batch 2,000  of  6,048.    Elapsed: 0:01:49.\n",
      "  Batch 2,050  of  6,048.    Elapsed: 0:01:52.\n",
      "  Batch 2,100  of  6,048.    Elapsed: 0:01:55.\n",
      "  Batch 2,150  of  6,048.    Elapsed: 0:01:57.\n",
      "  Batch 2,200  of  6,048.    Elapsed: 0:02:00.\n",
      "  Batch 2,250  of  6,048.    Elapsed: 0:02:03.\n",
      "  Batch 2,300  of  6,048.    Elapsed: 0:02:05.\n",
      "  Batch 2,350  of  6,048.    Elapsed: 0:02:08.\n",
      "  Batch 2,400  of  6,048.    Elapsed: 0:02:11.\n",
      "  Batch 2,450  of  6,048.    Elapsed: 0:02:14.\n",
      "  Batch 2,500  of  6,048.    Elapsed: 0:02:17.\n",
      "  Batch 2,550  of  6,048.    Elapsed: 0:02:19.\n",
      "  Batch 2,600  of  6,048.    Elapsed: 0:02:22.\n",
      "  Batch 2,650  of  6,048.    Elapsed: 0:02:25.\n",
      "  Batch 2,700  of  6,048.    Elapsed: 0:02:27.\n",
      "  Batch 2,750  of  6,048.    Elapsed: 0:02:30.\n",
      "  Batch 2,800  of  6,048.    Elapsed: 0:02:33.\n",
      "  Batch 2,850  of  6,048.    Elapsed: 0:02:36.\n",
      "  Batch 2,900  of  6,048.    Elapsed: 0:02:38.\n",
      "  Batch 2,950  of  6,048.    Elapsed: 0:02:41.\n",
      "  Batch 3,000  of  6,048.    Elapsed: 0:02:44.\n",
      "  Batch 3,050  of  6,048.    Elapsed: 0:02:46.\n",
      "  Batch 3,100  of  6,048.    Elapsed: 0:02:49.\n",
      "  Batch 3,150  of  6,048.    Elapsed: 0:02:52.\n",
      "  Batch 3,200  of  6,048.    Elapsed: 0:02:54.\n",
      "  Batch 3,250  of  6,048.    Elapsed: 0:02:57.\n",
      "  Batch 3,300  of  6,048.    Elapsed: 0:02:59.\n",
      "  Batch 3,350  of  6,048.    Elapsed: 0:03:02.\n",
      "  Batch 3,400  of  6,048.    Elapsed: 0:03:05.\n",
      "  Batch 3,450  of  6,048.    Elapsed: 0:03:07.\n",
      "  Batch 3,500  of  6,048.    Elapsed: 0:03:10.\n",
      "  Batch 3,550  of  6,048.    Elapsed: 0:03:12.\n",
      "  Batch 3,600  of  6,048.    Elapsed: 0:03:15.\n",
      "  Batch 3,650  of  6,048.    Elapsed: 0:03:18.\n",
      "  Batch 3,700  of  6,048.    Elapsed: 0:03:20.\n",
      "  Batch 3,750  of  6,048.    Elapsed: 0:03:23.\n",
      "  Batch 3,800  of  6,048.    Elapsed: 0:03:26.\n",
      "  Batch 3,850  of  6,048.    Elapsed: 0:03:29.\n",
      "  Batch 3,900  of  6,048.    Elapsed: 0:03:31.\n",
      "  Batch 3,950  of  6,048.    Elapsed: 0:03:34.\n",
      "  Batch 4,000  of  6,048.    Elapsed: 0:03:37.\n",
      "  Batch 4,050  of  6,048.    Elapsed: 0:03:39.\n",
      "  Batch 4,100  of  6,048.    Elapsed: 0:03:42.\n",
      "  Batch 4,150  of  6,048.    Elapsed: 0:03:45.\n",
      "  Batch 4,200  of  6,048.    Elapsed: 0:03:48.\n",
      "  Batch 4,250  of  6,048.    Elapsed: 0:03:50.\n",
      "  Batch 4,300  of  6,048.    Elapsed: 0:03:53.\n",
      "  Batch 4,350  of  6,048.    Elapsed: 0:03:55.\n",
      "  Batch 4,400  of  6,048.    Elapsed: 0:03:58.\n",
      "  Batch 4,450  of  6,048.    Elapsed: 0:04:01.\n",
      "  Batch 4,500  of  6,048.    Elapsed: 0:04:03.\n",
      "  Batch 4,550  of  6,048.    Elapsed: 0:04:06.\n",
      "  Batch 4,600  of  6,048.    Elapsed: 0:04:09.\n",
      "  Batch 4,650  of  6,048.    Elapsed: 0:04:11.\n",
      "  Batch 4,700  of  6,048.    Elapsed: 0:04:14.\n",
      "  Batch 4,750  of  6,048.    Elapsed: 0:04:16.\n",
      "  Batch 4,800  of  6,048.    Elapsed: 0:04:19.\n",
      "  Batch 4,850  of  6,048.    Elapsed: 0:04:22.\n",
      "  Batch 4,900  of  6,048.    Elapsed: 0:04:24.\n",
      "  Batch 4,950  of  6,048.    Elapsed: 0:04:27.\n",
      "  Batch 5,000  of  6,048.    Elapsed: 0:04:30.\n",
      "  Batch 5,050  of  6,048.    Elapsed: 0:04:32.\n",
      "  Batch 5,100  of  6,048.    Elapsed: 0:04:35.\n",
      "  Batch 5,150  of  6,048.    Elapsed: 0:04:37.\n",
      "  Batch 5,200  of  6,048.    Elapsed: 0:04:40.\n",
      "  Batch 5,250  of  6,048.    Elapsed: 0:04:43.\n",
      "  Batch 5,300  of  6,048.    Elapsed: 0:04:45.\n",
      "  Batch 5,350  of  6,048.    Elapsed: 0:04:48.\n",
      "  Batch 5,400  of  6,048.    Elapsed: 0:04:51.\n",
      "  Batch 5,450  of  6,048.    Elapsed: 0:04:53.\n",
      "  Batch 5,500  of  6,048.    Elapsed: 0:04:56.\n",
      "  Batch 5,550  of  6,048.    Elapsed: 0:04:59.\n",
      "  Batch 5,600  of  6,048.    Elapsed: 0:05:01.\n",
      "  Batch 5,650  of  6,048.    Elapsed: 0:05:04.\n",
      "  Batch 5,700  of  6,048.    Elapsed: 0:05:06.\n",
      "  Batch 5,750  of  6,048.    Elapsed: 0:05:09.\n",
      "  Batch 5,800  of  6,048.    Elapsed: 0:05:12.\n",
      "  Batch 5,850  of  6,048.    Elapsed: 0:05:15.\n",
      "  Batch 5,900  of  6,048.    Elapsed: 0:05:17.\n",
      "  Batch 5,950  of  6,048.    Elapsed: 0:05:20.\n",
      "  Batch 6,000  of  6,048.    Elapsed: 0:05:23.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoch took: 0:05:25\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    50  of  6,048.    Elapsed: 0:00:03.\n",
      "  Batch   100  of  6,048.    Elapsed: 0:00:05.\n",
      "  Batch   150  of  6,048.    Elapsed: 0:00:08.\n",
      "  Batch   200  of  6,048.    Elapsed: 0:00:11.\n",
      "  Batch   250  of  6,048.    Elapsed: 0:00:14.\n",
      "  Batch   300  of  6,048.    Elapsed: 0:00:16.\n",
      "  Batch   350  of  6,048.    Elapsed: 0:00:19.\n",
      "  Batch   400  of  6,048.    Elapsed: 0:00:22.\n",
      "  Batch   450  of  6,048.    Elapsed: 0:00:25.\n",
      "  Batch   500  of  6,048.    Elapsed: 0:00:27.\n",
      "  Batch   550  of  6,048.    Elapsed: 0:00:30.\n",
      "  Batch   600  of  6,048.    Elapsed: 0:00:33.\n",
      "  Batch   650  of  6,048.    Elapsed: 0:00:35.\n",
      "  Batch   700  of  6,048.    Elapsed: 0:00:38.\n",
      "  Batch   750  of  6,048.    Elapsed: 0:00:41.\n",
      "  Batch   800  of  6,048.    Elapsed: 0:00:44.\n",
      "  Batch   850  of  6,048.    Elapsed: 0:00:46.\n",
      "  Batch   900  of  6,048.    Elapsed: 0:00:49.\n",
      "  Batch   950  of  6,048.    Elapsed: 0:00:52.\n",
      "  Batch 1,000  of  6,048.    Elapsed: 0:00:54.\n",
      "  Batch 1,050  of  6,048.    Elapsed: 0:00:57.\n",
      "  Batch 1,100  of  6,048.    Elapsed: 0:01:00.\n",
      "  Batch 1,150  of  6,048.    Elapsed: 0:01:03.\n",
      "  Batch 1,200  of  6,048.    Elapsed: 0:01:05.\n",
      "  Batch 1,250  of  6,048.    Elapsed: 0:01:08.\n",
      "  Batch 1,300  of  6,048.    Elapsed: 0:01:11.\n",
      "  Batch 1,350  of  6,048.    Elapsed: 0:01:13.\n",
      "  Batch 1,400  of  6,048.    Elapsed: 0:01:16.\n",
      "  Batch 1,450  of  6,048.    Elapsed: 0:01:19.\n",
      "  Batch 1,500  of  6,048.    Elapsed: 0:01:22.\n",
      "  Batch 1,550  of  6,048.    Elapsed: 0:01:24.\n",
      "  Batch 1,600  of  6,048.    Elapsed: 0:01:27.\n",
      "  Batch 1,650  of  6,048.    Elapsed: 0:01:30.\n",
      "  Batch 1,700  of  6,048.    Elapsed: 0:01:33.\n",
      "  Batch 1,750  of  6,048.    Elapsed: 0:01:35.\n",
      "  Batch 1,800  of  6,048.    Elapsed: 0:01:38.\n",
      "  Batch 1,850  of  6,048.    Elapsed: 0:01:41.\n",
      "  Batch 1,900  of  6,048.    Elapsed: 0:01:44.\n",
      "  Batch 1,950  of  6,048.    Elapsed: 0:01:46.\n",
      "  Batch 2,000  of  6,048.    Elapsed: 0:01:49.\n",
      "  Batch 2,050  of  6,048.    Elapsed: 0:01:52.\n",
      "  Batch 2,100  of  6,048.    Elapsed: 0:01:54.\n",
      "  Batch 2,150  of  6,048.    Elapsed: 0:01:57.\n",
      "  Batch 2,200  of  6,048.    Elapsed: 0:02:00.\n",
      "  Batch 2,250  of  6,048.    Elapsed: 0:02:03.\n",
      "  Batch 2,300  of  6,048.    Elapsed: 0:02:05.\n",
      "  Batch 2,350  of  6,048.    Elapsed: 0:02:08.\n",
      "  Batch 2,400  of  6,048.    Elapsed: 0:02:11.\n",
      "  Batch 2,450  of  6,048.    Elapsed: 0:02:14.\n",
      "  Batch 2,500  of  6,048.    Elapsed: 0:02:16.\n",
      "  Batch 2,550  of  6,048.    Elapsed: 0:02:19.\n",
      "  Batch 2,600  of  6,048.    Elapsed: 0:02:22.\n",
      "  Batch 2,650  of  6,048.    Elapsed: 0:02:25.\n",
      "  Batch 2,700  of  6,048.    Elapsed: 0:02:27.\n",
      "  Batch 2,750  of  6,048.    Elapsed: 0:02:30.\n",
      "  Batch 2,800  of  6,048.    Elapsed: 0:02:33.\n",
      "  Batch 2,850  of  6,048.    Elapsed: 0:02:35.\n",
      "  Batch 2,900  of  6,048.    Elapsed: 0:02:38.\n",
      "  Batch 2,950  of  6,048.    Elapsed: 0:02:41.\n",
      "  Batch 3,000  of  6,048.    Elapsed: 0:02:44.\n",
      "  Batch 3,050  of  6,048.    Elapsed: 0:02:46.\n",
      "  Batch 3,100  of  6,048.    Elapsed: 0:02:49.\n",
      "  Batch 3,150  of  6,048.    Elapsed: 0:02:52.\n",
      "  Batch 3,200  of  6,048.    Elapsed: 0:02:55.\n",
      "  Batch 3,250  of  6,048.    Elapsed: 0:02:57.\n",
      "  Batch 3,300  of  6,048.    Elapsed: 0:03:00.\n",
      "  Batch 3,350  of  6,048.    Elapsed: 0:03:03.\n",
      "  Batch 3,400  of  6,048.    Elapsed: 0:03:06.\n",
      "  Batch 3,450  of  6,048.    Elapsed: 0:03:08.\n",
      "  Batch 3,500  of  6,048.    Elapsed: 0:03:11.\n",
      "  Batch 3,550  of  6,048.    Elapsed: 0:03:14.\n",
      "  Batch 3,600  of  6,048.    Elapsed: 0:03:17.\n",
      "  Batch 3,650  of  6,048.    Elapsed: 0:03:20.\n",
      "  Batch 3,700  of  6,048.    Elapsed: 0:03:22.\n",
      "  Batch 3,750  of  6,048.    Elapsed: 0:03:25.\n",
      "  Batch 3,800  of  6,048.    Elapsed: 0:03:28.\n",
      "  Batch 3,850  of  6,048.    Elapsed: 0:03:31.\n",
      "  Batch 3,900  of  6,048.    Elapsed: 0:03:33.\n",
      "  Batch 3,950  of  6,048.    Elapsed: 0:03:36.\n",
      "  Batch 4,000  of  6,048.    Elapsed: 0:03:39.\n",
      "  Batch 4,050  of  6,048.    Elapsed: 0:03:42.\n",
      "  Batch 4,100  of  6,048.    Elapsed: 0:03:44.\n",
      "  Batch 4,150  of  6,048.    Elapsed: 0:03:47.\n",
      "  Batch 4,200  of  6,048.    Elapsed: 0:03:50.\n",
      "  Batch 4,250  of  6,048.    Elapsed: 0:03:53.\n",
      "  Batch 4,300  of  6,048.    Elapsed: 0:03:56.\n",
      "  Batch 4,350  of  6,048.    Elapsed: 0:03:58.\n",
      "  Batch 4,400  of  6,048.    Elapsed: 0:04:01.\n",
      "  Batch 4,450  of  6,048.    Elapsed: 0:04:04.\n",
      "  Batch 4,500  of  6,048.    Elapsed: 0:04:07.\n",
      "  Batch 4,550  of  6,048.    Elapsed: 0:04:09.\n",
      "  Batch 4,600  of  6,048.    Elapsed: 0:04:12.\n",
      "  Batch 4,650  of  6,048.    Elapsed: 0:04:15.\n",
      "  Batch 4,700  of  6,048.    Elapsed: 0:04:18.\n",
      "  Batch 4,750  of  6,048.    Elapsed: 0:04:21.\n",
      "  Batch 4,800  of  6,048.    Elapsed: 0:04:23.\n",
      "  Batch 4,850  of  6,048.    Elapsed: 0:04:26.\n",
      "  Batch 4,900  of  6,048.    Elapsed: 0:04:29.\n",
      "  Batch 4,950  of  6,048.    Elapsed: 0:04:32.\n",
      "  Batch 5,000  of  6,048.    Elapsed: 0:04:35.\n",
      "  Batch 5,050  of  6,048.    Elapsed: 0:04:37.\n",
      "  Batch 5,100  of  6,048.    Elapsed: 0:04:40.\n",
      "  Batch 5,150  of  6,048.    Elapsed: 0:04:43.\n",
      "  Batch 5,200  of  6,048.    Elapsed: 0:04:46.\n",
      "  Batch 5,250  of  6,048.    Elapsed: 0:04:49.\n",
      "  Batch 5,300  of  6,048.    Elapsed: 0:04:51.\n",
      "  Batch 5,350  of  6,048.    Elapsed: 0:04:54.\n",
      "  Batch 5,400  of  6,048.    Elapsed: 0:04:57.\n",
      "  Batch 5,450  of  6,048.    Elapsed: 0:05:00.\n",
      "  Batch 5,500  of  6,048.    Elapsed: 0:05:02.\n",
      "  Batch 5,550  of  6,048.    Elapsed: 0:05:05.\n",
      "  Batch 5,600  of  6,048.    Elapsed: 0:05:08.\n",
      "  Batch 5,650  of  6,048.    Elapsed: 0:05:10.\n",
      "  Batch 5,700  of  6,048.    Elapsed: 0:05:13.\n",
      "  Batch 5,750  of  6,048.    Elapsed: 0:05:16.\n",
      "  Batch 5,800  of  6,048.    Elapsed: 0:05:19.\n",
      "  Batch 5,850  of  6,048.    Elapsed: 0:05:21.\n",
      "  Batch 5,900  of  6,048.    Elapsed: 0:05:24.\n",
      "  Batch 5,950  of  6,048.    Elapsed: 0:05:27.\n",
      "  Batch 6,000  of  6,048.    Elapsed: 0:05:30.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoch took: 0:05:32\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "loss_values = []\n",
    "for epoch_i in range(epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        model.zero_grad()\n",
    "        outputs = model(b_input_ids,\n",
    "                       token_type_ids=None,\n",
    "                       attention_mask=b_input_mask,\n",
    "                       labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        total_loss == loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    loss_values.append(avg_train_loss)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "      \n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.81\n",
      "  Validation took: 0:00:13\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(\"Running Validation...\")\n",
    "\n",
    "t0 = time.time()\n",
    "model.eval()\n",
    "preds = []\n",
    "true = []\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "for batch in validation_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids,\n",
    "                       token_type_ids=None,\n",
    "                       attention_mask=b_input_mask)\n",
    "        \n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    preds.append(logits)\n",
    "    true.append(label_ids)\n",
    "    \n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "    \n",
    "print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_predictions = [item for sublist in preds for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "flat_true_labels = [item for sublist in true for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.83      0.84       891\n",
      "          1       0.76      0.79      0.78       622\n",
      "\n",
      "avg / total       0.81      0.81      0.81      1513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(flat_predictions, flat_true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/herokuma/miniconda3/envs/pytorch/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "comments1 = df_test.text.values\n",
    "\n",
    "indices1=tokenizer.batch_encode_plus(comments1,max_length=128,add_special_tokens=True, return_attention_mask=True,pad_to_max_length=True,truncation=True)\n",
    "input_ids1=indices1[\"input_ids\"]\n",
    "attention_masks1=indices1[\"attention_mask\"]\n",
    "\n",
    "prediction_inputs1= torch.tensor(input_ids1)\n",
    "prediction_masks1 = torch.tensor(attention_masks1)\n",
    "\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32 \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data1 = TensorDataset(prediction_inputs1, prediction_masks1)\n",
    "prediction_sampler1 = SequentialSampler(prediction_data1)\n",
    "prediction_dataloader1 = DataLoader(prediction_data1, sampler=prediction_sampler1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 3,263 test sentences...\n"
     ]
    }
   ],
   "source": [
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs1)))\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "for batch in prediction_dataloader1:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids1, b_input_mask1 = batch\n",
    "    with torch.no_grad():\n",
    "        outputs1 = model(b_input_ids1, token_type_ids=None, attention_mask=b_input_mask1)\n",
    "    \n",
    "    logits1 = outputs1[0]\n",
    "    logits1 = logits1.detach().cpu().numpy()\n",
    "    predictions.append(logits1)\n",
    "    \n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv('./sample_submission.csv')\n",
    "submit = pd.DataFrame({'id': sample_sub['id'].values.tolist(), 'target': flat_predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaked Data Set Shape = (10876, 2)\n",
      "Leaked Data Set Memory Usage = 0.09 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3263.000000</td>\n",
       "      <td>3263.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5427.152927</td>\n",
       "      <td>0.429666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3146.427221</td>\n",
       "      <td>0.495104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2683.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5500.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8176.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10875.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id       target\n",
       "count   3263.000000  3263.000000\n",
       "mean    5427.152927     0.429666\n",
       "std     3146.427221     0.495104\n",
       "min        0.000000     0.000000\n",
       "25%     2683.000000     0.000000\n",
       "50%     5500.000000     0.000000\n",
       "75%     8176.000000     1.000000\n",
       "max    10875.000000     1.000000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_leak = pd.read_csv('./socialmedia-disaster-tweets-DFE.csv', encoding ='ISO-8859-1')[['choose_one', 'text']]\n",
    "\n",
    "# Creating target and id\n",
    "df_leak['target'] = (df_leak['choose_one'] == 'Relevant').astype(np.int8)\n",
    "df_leak['id'] = df_leak.index.astype(np.int16)\n",
    "df_leak.drop(columns=['choose_one', 'text'], inplace=True)\n",
    "\n",
    "# Merging target to test set\n",
    "df_test = df_test.merge(df_leak, on=['id'], how='left')\n",
    "\n",
    "print('Leaked Data Set Shape = {}'.format(df_leak.shape))\n",
    "print('Leaked Data Set Memory Usage = {:.2f} MB'.format(df_leak.memory_usage().sum() / 1024**2))\n",
    "\n",
    "perfect_submission = pd.read_csv(\"./sample_submission.csv\")\n",
    "perfect_submission['target'] = df_test['target'].values\n",
    "perfect_submission.to_csv('submission.csv', index=False)\n",
    "perfect_submission.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
